{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictor</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>...</th>\n",
       "      <th>1061</th>\n",
       "      <th>1062</th>\n",
       "      <th>1063</th>\n",
       "      <th>1064</th>\n",
       "      <th>1065</th>\n",
       "      <th>1066</th>\n",
       "      <th>1067</th>\n",
       "      <th>1068</th>\n",
       "      <th>1069</th>\n",
       "      <th>1070</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HB-52</td>\n",
       "      <td>1.499853</td>\n",
       "      <td>1.500665</td>\n",
       "      <td>1.501564</td>\n",
       "      <td>1.502536</td>\n",
       "      <td>1.503537</td>\n",
       "      <td>1.504548</td>\n",
       "      <td>1.505588</td>\n",
       "      <td>1.506689</td>\n",
       "      <td>1.507828</td>\n",
       "      <td>...</td>\n",
       "      <td>1.636411</td>\n",
       "      <td>1.637350</td>\n",
       "      <td>1.636452</td>\n",
       "      <td>1.634370</td>\n",
       "      <td>1.632287</td>\n",
       "      <td>1.630265</td>\n",
       "      <td>1.630722</td>\n",
       "      <td>1.633166</td>\n",
       "      <td>1.636204</td>\n",
       "      <td>1.637872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HB-52</td>\n",
       "      <td>1.530877</td>\n",
       "      <td>1.531669</td>\n",
       "      <td>1.532524</td>\n",
       "      <td>1.533419</td>\n",
       "      <td>1.534308</td>\n",
       "      <td>1.535169</td>\n",
       "      <td>1.536024</td>\n",
       "      <td>1.536916</td>\n",
       "      <td>1.537830</td>\n",
       "      <td>...</td>\n",
       "      <td>1.643299</td>\n",
       "      <td>1.644222</td>\n",
       "      <td>1.643302</td>\n",
       "      <td>1.641195</td>\n",
       "      <td>1.639088</td>\n",
       "      <td>1.637046</td>\n",
       "      <td>1.637493</td>\n",
       "      <td>1.639938</td>\n",
       "      <td>1.642981</td>\n",
       "      <td>1.644649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HB-52</td>\n",
       "      <td>1.607175</td>\n",
       "      <td>1.608642</td>\n",
       "      <td>1.610044</td>\n",
       "      <td>1.611357</td>\n",
       "      <td>1.612540</td>\n",
       "      <td>1.613585</td>\n",
       "      <td>1.614534</td>\n",
       "      <td>1.615456</td>\n",
       "      <td>1.616359</td>\n",
       "      <td>...</td>\n",
       "      <td>1.703343</td>\n",
       "      <td>1.704291</td>\n",
       "      <td>1.703329</td>\n",
       "      <td>1.701137</td>\n",
       "      <td>1.698946</td>\n",
       "      <td>1.696822</td>\n",
       "      <td>1.697278</td>\n",
       "      <td>1.699806</td>\n",
       "      <td>1.702953</td>\n",
       "      <td>1.704677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HB-52</td>\n",
       "      <td>1.677333</td>\n",
       "      <td>1.678098</td>\n",
       "      <td>1.678930</td>\n",
       "      <td>1.679809</td>\n",
       "      <td>1.680688</td>\n",
       "      <td>1.681539</td>\n",
       "      <td>1.682384</td>\n",
       "      <td>1.683260</td>\n",
       "      <td>1.684145</td>\n",
       "      <td>...</td>\n",
       "      <td>1.755067</td>\n",
       "      <td>1.756013</td>\n",
       "      <td>1.754996</td>\n",
       "      <td>1.752716</td>\n",
       "      <td>1.750441</td>\n",
       "      <td>1.748238</td>\n",
       "      <td>1.748696</td>\n",
       "      <td>1.751290</td>\n",
       "      <td>1.754525</td>\n",
       "      <td>1.756294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HB-52</td>\n",
       "      <td>1.507830</td>\n",
       "      <td>1.509293</td>\n",
       "      <td>1.510683</td>\n",
       "      <td>1.511977</td>\n",
       "      <td>1.513145</td>\n",
       "      <td>1.514180</td>\n",
       "      <td>1.515125</td>\n",
       "      <td>1.516049</td>\n",
       "      <td>1.516964</td>\n",
       "      <td>...</td>\n",
       "      <td>1.641968</td>\n",
       "      <td>1.642877</td>\n",
       "      <td>1.641946</td>\n",
       "      <td>1.639829</td>\n",
       "      <td>1.637714</td>\n",
       "      <td>1.635664</td>\n",
       "      <td>1.636101</td>\n",
       "      <td>1.638535</td>\n",
       "      <td>1.641567</td>\n",
       "      <td>1.643227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 332 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Predictor       740       741       742       743       744       745  \\\n",
       "0     HB-52  1.499853  1.500665  1.501564  1.502536  1.503537  1.504548   \n",
       "1     HB-52  1.530877  1.531669  1.532524  1.533419  1.534308  1.535169   \n",
       "2     HB-52  1.607175  1.608642  1.610044  1.611357  1.612540  1.613585   \n",
       "3     HB-52  1.677333  1.678098  1.678930  1.679809  1.680688  1.681539   \n",
       "4     HB-52  1.507830  1.509293  1.510683  1.511977  1.513145  1.514180   \n",
       "\n",
       "        746       747       748    ...         1061      1062      1063  \\\n",
       "0  1.505588  1.506689  1.507828    ...     1.636411  1.637350  1.636452   \n",
       "1  1.536024  1.536916  1.537830    ...     1.643299  1.644222  1.643302   \n",
       "2  1.614534  1.615456  1.616359    ...     1.703343  1.704291  1.703329   \n",
       "3  1.682384  1.683260  1.684145    ...     1.755067  1.756013  1.754996   \n",
       "4  1.515125  1.516049  1.516964    ...     1.641968  1.642877  1.641946   \n",
       "\n",
       "       1064      1065      1066      1067      1068      1069      1070  \n",
       "0  1.634370  1.632287  1.630265  1.630722  1.633166  1.636204  1.637872  \n",
       "1  1.641195  1.639088  1.637046  1.637493  1.639938  1.642981  1.644649  \n",
       "2  1.701137  1.698946  1.696822  1.697278  1.699806  1.702953  1.704677  \n",
       "3  1.752716  1.750441  1.748238  1.748696  1.751290  1.754525  1.756294  \n",
       "4  1.639829  1.637714  1.635664  1.636101  1.638535  1.641567  1.643227  \n",
       "\n",
       "[5 rows x 332 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the csv file and putting it into 'df' object.\n",
    "df = pd.read_csv(\"M:\\Imarticus\\python project\\crop-varietal-identification-with-scio\\Barley.data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>...</th>\n",
       "      <th>1061</th>\n",
       "      <th>1062</th>\n",
       "      <th>1063</th>\n",
       "      <th>1064</th>\n",
       "      <th>1065</th>\n",
       "      <th>1066</th>\n",
       "      <th>1067</th>\n",
       "      <th>1068</th>\n",
       "      <th>1069</th>\n",
       "      <th>1070</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.499853</td>\n",
       "      <td>1.500665</td>\n",
       "      <td>1.501564</td>\n",
       "      <td>1.502536</td>\n",
       "      <td>1.503537</td>\n",
       "      <td>1.504548</td>\n",
       "      <td>1.505588</td>\n",
       "      <td>1.506689</td>\n",
       "      <td>1.507828</td>\n",
       "      <td>1.508937</td>\n",
       "      <td>...</td>\n",
       "      <td>1.636411</td>\n",
       "      <td>1.637350</td>\n",
       "      <td>1.636452</td>\n",
       "      <td>1.634370</td>\n",
       "      <td>1.632287</td>\n",
       "      <td>1.630265</td>\n",
       "      <td>1.630722</td>\n",
       "      <td>1.633166</td>\n",
       "      <td>1.636204</td>\n",
       "      <td>1.637872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.530877</td>\n",
       "      <td>1.531669</td>\n",
       "      <td>1.532524</td>\n",
       "      <td>1.533419</td>\n",
       "      <td>1.534308</td>\n",
       "      <td>1.535169</td>\n",
       "      <td>1.536024</td>\n",
       "      <td>1.536916</td>\n",
       "      <td>1.537830</td>\n",
       "      <td>1.538712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.643299</td>\n",
       "      <td>1.644222</td>\n",
       "      <td>1.643302</td>\n",
       "      <td>1.641195</td>\n",
       "      <td>1.639088</td>\n",
       "      <td>1.637046</td>\n",
       "      <td>1.637493</td>\n",
       "      <td>1.639938</td>\n",
       "      <td>1.642981</td>\n",
       "      <td>1.644649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.607175</td>\n",
       "      <td>1.608642</td>\n",
       "      <td>1.610044</td>\n",
       "      <td>1.611357</td>\n",
       "      <td>1.612540</td>\n",
       "      <td>1.613585</td>\n",
       "      <td>1.614534</td>\n",
       "      <td>1.615456</td>\n",
       "      <td>1.616359</td>\n",
       "      <td>1.617205</td>\n",
       "      <td>...</td>\n",
       "      <td>1.703343</td>\n",
       "      <td>1.704291</td>\n",
       "      <td>1.703329</td>\n",
       "      <td>1.701137</td>\n",
       "      <td>1.698946</td>\n",
       "      <td>1.696822</td>\n",
       "      <td>1.697278</td>\n",
       "      <td>1.699806</td>\n",
       "      <td>1.702953</td>\n",
       "      <td>1.704677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.677333</td>\n",
       "      <td>1.678098</td>\n",
       "      <td>1.678930</td>\n",
       "      <td>1.679809</td>\n",
       "      <td>1.680688</td>\n",
       "      <td>1.681539</td>\n",
       "      <td>1.682384</td>\n",
       "      <td>1.683260</td>\n",
       "      <td>1.684145</td>\n",
       "      <td>1.684969</td>\n",
       "      <td>...</td>\n",
       "      <td>1.755067</td>\n",
       "      <td>1.756013</td>\n",
       "      <td>1.754996</td>\n",
       "      <td>1.752716</td>\n",
       "      <td>1.750441</td>\n",
       "      <td>1.748238</td>\n",
       "      <td>1.748696</td>\n",
       "      <td>1.751290</td>\n",
       "      <td>1.754525</td>\n",
       "      <td>1.756294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.507830</td>\n",
       "      <td>1.509293</td>\n",
       "      <td>1.510683</td>\n",
       "      <td>1.511977</td>\n",
       "      <td>1.513145</td>\n",
       "      <td>1.514180</td>\n",
       "      <td>1.515125</td>\n",
       "      <td>1.516049</td>\n",
       "      <td>1.516964</td>\n",
       "      <td>1.517836</td>\n",
       "      <td>...</td>\n",
       "      <td>1.641968</td>\n",
       "      <td>1.642877</td>\n",
       "      <td>1.641946</td>\n",
       "      <td>1.639829</td>\n",
       "      <td>1.637714</td>\n",
       "      <td>1.635664</td>\n",
       "      <td>1.636101</td>\n",
       "      <td>1.638535</td>\n",
       "      <td>1.641567</td>\n",
       "      <td>1.643227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        740       741       742       743       744       745       746  \\\n",
       "0  1.499853  1.500665  1.501564  1.502536  1.503537  1.504548  1.505588   \n",
       "1  1.530877  1.531669  1.532524  1.533419  1.534308  1.535169  1.536024   \n",
       "2  1.607175  1.608642  1.610044  1.611357  1.612540  1.613585  1.614534   \n",
       "3  1.677333  1.678098  1.678930  1.679809  1.680688  1.681539  1.682384   \n",
       "4  1.507830  1.509293  1.510683  1.511977  1.513145  1.514180  1.515125   \n",
       "\n",
       "        747       748       749    ...         1061      1062      1063  \\\n",
       "0  1.506689  1.507828  1.508937    ...     1.636411  1.637350  1.636452   \n",
       "1  1.536916  1.537830  1.538712    ...     1.643299  1.644222  1.643302   \n",
       "2  1.615456  1.616359  1.617205    ...     1.703343  1.704291  1.703329   \n",
       "3  1.683260  1.684145  1.684969    ...     1.755067  1.756013  1.754996   \n",
       "4  1.516049  1.516964  1.517836    ...     1.641968  1.642877  1.641946   \n",
       "\n",
       "       1064      1065      1066      1067      1068      1069      1070  \n",
       "0  1.634370  1.632287  1.630265  1.630722  1.633166  1.636204  1.637872  \n",
       "1  1.641195  1.639088  1.637046  1.637493  1.639938  1.642981  1.644649  \n",
       "2  1.701137  1.698946  1.696822  1.697278  1.699806  1.702953  1.704677  \n",
       "3  1.752716  1.750441  1.748238  1.748696  1.751290  1.754525  1.756294  \n",
       "4  1.639829  1.637714  1.635664  1.636101  1.638535  1.641567  1.643227  \n",
       "\n",
       "[5 rows x 331 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collecting indepedent variables in X\n",
    "X = df.iloc[:,1:332]\n",
    "X_col = X.columns\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting depedent variable in Y\n",
    "Y=df['Predictor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing methods are helpful in eliminating noise generated by spectral data. Raw spectral data were thus processed using a combination of scatter corrections that include Standard Normal Variate (SNV) as well as first and second degree derivatives. The Savitzky-Golay and Gap-segment derivative smoothing filtering algorithms also usefull in eliminating noise.\n",
    "\n",
    "Here I am using Savitzky-Golay filter with second degree derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Savitzky-Golay filter with second degree derivative.\n",
    "from scipy.signal import savgol_filter \n",
    "\n",
    "sg=savgol_filter(X,window_length=11, polyorder=3, deriv=2, delta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>...</th>\n",
       "      <th>1061</th>\n",
       "      <th>1062</th>\n",
       "      <th>1063</th>\n",
       "      <th>1064</th>\n",
       "      <th>1065</th>\n",
       "      <th>1066</th>\n",
       "      <th>1067</th>\n",
       "      <th>1068</th>\n",
       "      <th>1069</th>\n",
       "      <th>1070</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000446</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000446</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.002642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000139</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.000073</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000462</td>\n",
       "      <td>-0.000491</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.002738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>-0.000502</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.002822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000055</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000446</td>\n",
       "      <td>-0.000473</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.002640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        740       741       742       743       744       745       746  \\\n",
       "0  0.000084  0.000073  0.000061  0.000050  0.000039  0.000028  0.000010   \n",
       "1  0.000033  0.000027  0.000021  0.000014  0.000008  0.000001 -0.000011   \n",
       "2 -0.000139 -0.000128 -0.000117 -0.000105 -0.000094 -0.000082 -0.000078   \n",
       "3  0.000055  0.000043  0.000031  0.000020  0.000008 -0.000004 -0.000022   \n",
       "4 -0.000148 -0.000133 -0.000119 -0.000105 -0.000091 -0.000077 -0.000069   \n",
       "\n",
       "        747       748       749    ...         1061      1062      1063  \\\n",
       "0 -0.000011 -0.000030 -0.000044    ...    -0.000446 -0.000474 -0.000300   \n",
       "1 -0.000024 -0.000036 -0.000042    ...    -0.000446 -0.000474 -0.000300   \n",
       "2 -0.000073 -0.000069 -0.000060    ...    -0.000462 -0.000491 -0.000310   \n",
       "3 -0.000043 -0.000061 -0.000071    ...    -0.000471 -0.000502 -0.000315   \n",
       "4 -0.000062 -0.000055 -0.000045    ...    -0.000446 -0.000473 -0.000299   \n",
       "\n",
       "       1064      1065      1066      1067      1068      1069      1070  \n",
       "0  0.000008  0.000319  0.000781  0.001243  0.001705  0.002167  0.002629  \n",
       "1  0.000010  0.000322  0.000786  0.001250  0.001714  0.002178  0.002642  \n",
       "2  0.000010  0.000334  0.000815  0.001296  0.001777  0.002258  0.002738  \n",
       "3  0.000014  0.000347  0.000842  0.001337  0.001832  0.002327  0.002822  \n",
       "4  0.000010  0.000322  0.000786  0.001249  0.001713  0.002176  0.002640  \n",
       "\n",
       "[5 rows x 331 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_x=pd.DataFrame(sg, columns=X_col)\n",
    "\n",
    "sg_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(sg, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Rf = RandomForestClassifier(random_state=52)\n",
    "Rf_fit=Rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result:\n",
      "\n",
      "accuracy score: 0.8083\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " Ardu 1260 B       0.89      0.80      0.84        10\n",
      "      Bahati       0.53      0.90      0.67        10\n",
      "    Bekoji-1       1.00      1.00      1.00        10\n",
      " Cross 41/98       0.91      1.00      0.95        10\n",
      "      Deribe       0.70      0.70      0.70        10\n",
      "       Dimtu       1.00      1.00      1.00        10\n",
      "     EH 1493       1.00      0.80      0.89        10\n",
      "     EH 1847       0.80      0.80      0.80        10\n",
      "    Explorer       0.75      0.90      0.82        10\n",
      "       Grace       0.67      0.60      0.63        10\n",
      "     HB-1307       0.75      0.90      0.82        10\n",
      "     HB-1533       0.62      0.50      0.56        10\n",
      "     HB-1963       1.00      0.90      0.95        10\n",
      "     HB-1964       1.00      1.00      1.00        10\n",
      "     HB-1965       0.80      0.80      0.80        10\n",
      "     HB-1966       0.62      0.50      0.56        10\n",
      " IBON 174/03       1.00      1.00      1.00        10\n",
      "      Sabini       0.78      0.70      0.74        10\n",
      "       Shege       0.50      0.40      0.44        10\n",
      "   Traveller       1.00      1.00      1.00        10\n",
      "        Beka       0.91      1.00      0.95        10\n",
      "       HB-52       0.75      0.90      0.82        10\n",
      "      Holker       0.75      0.60      0.67        10\n",
      "  Misccal-21       0.78      0.70      0.74        10\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       240\n",
      "   macro avg       0.81      0.81      0.81       240\n",
      "weighted avg       0.81      0.81      0.81       240\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 8  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  7  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  1  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  1  2  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  5  0  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  2]\n",
      " [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0  1  3  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  9  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  1  0  6  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  7]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Test Result:\\n\")        \n",
    "print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, y_pred)))\n",
    "print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test,y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      "  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "#Reduction of variables using Recursive Feature Elimination(RFE) techineque\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# RFE with 10 features\n",
    "\n",
    "rfe_10 = RFE(Rf,10)\n",
    "\n",
    "rfe_10.fit(X_train, y_train)\n",
    "\n",
    "# Printing the boolean results\n",
    "print(rfe_10.support_)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.7250\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 8  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  8  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  7  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  9  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  2  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  1  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  8  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  5  0  0  0  0  0  1  0  0  0  0  3  0]\n",
      " [ 0  0  0  0  2  0  1  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  2  0  0  0  2  0  0  0  0  5  0  0  1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  4  0  0  0  1  0  0  1  3  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  1  0  0  1  5  1]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  1  0  0  0  5]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfe_10.predict(X_test)\n",
    "\n",
    "print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False  True False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True False False False False False False False False\n",
      " False False False False False False False False False  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      "  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# RFE with 15 features\n",
    "rfe_15 = RFE(Rf,15)\n",
    "\n",
    "# fit with 15 features\n",
    "rfe_15.fit(X_train, y_train)\n",
    "\n",
    "# Printing the boolean results\n",
    "print(rfe_15.support_)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.7292\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 8  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  9  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  7  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  6  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  2  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  9  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  2  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  6  0  0  0  0  0  0  2  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2  1  0  0  0  5  0  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  4]\n",
      " [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  4  0  0  1  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  2  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  1  0  0  0  0  7  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  6]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfe_15.predict(X_test)\n",
    "\n",
    "print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False  True False False  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True False False False False\n",
      " False False False False False False  True False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True False False False False False False False False\n",
      " False False False False False False False False False  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      "  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# RFE with 17 features\n",
    "\n",
    "rfe_17 = RFE(Rf,17)\n",
    "\n",
    "# fit with 17 features\n",
    "rfe_17.fit(X_train, y_train)\n",
    "\n",
    "# Printing the boolean results\n",
    "print(rfe_17.support_)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.7750\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 8  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  7  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  7  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  2  0  0  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  2  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  6  0  0  2  0  0  0  0  0]\n",
      " [ 1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  6]\n",
      " [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  3  0  0  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0  1  1  0  0  0  6  0]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  7]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfe_17.predict(X_test)\n",
    "\n",
    "print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False  True False False  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False  True  True False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True  True False False False False\n",
      " False False False False False False  True False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True False False False False False False  True False\n",
      " False False False False False False False False False  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      "  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# RFE with 20 features\n",
    "\n",
    "rfe_20 = RFE(Rf,20)\n",
    "\n",
    "# fit with 20 features\n",
    "rfe_20.fit(X_train, y_train)\n",
    "\n",
    "# Printing the boolean results\n",
    "print(rfe_20.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.7250\n",
      "\n",
      "Confusion Matrix: \n",
      " [[ 8  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  6  0  0  0  0  1  0  0  1  0  0  0  0  0  0  1  0  0  0  1  0  0  0]\n",
      " [ 0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  5  1  0  0  2  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  4  0  0  0  5  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  1  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  9  0  0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  1  1  0  0  0  2  0  0  0  0  5  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  4  0  0  0  0  0  0  1  4  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  6  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  1  0  6  1]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  5]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfe_20.predict(X_test)\n",
    "\n",
    "print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: 757                  Importance: 0.0161\n",
      "Variable: 790                  Importance: 0.0141\n",
      "Variable: 1059                 Importance: 0.014\n",
      "Variable: 848                  Importance: 0.0106\n",
      "Variable: 1065                 Importance: 0.0105\n",
      "Variable: 975                  Importance: 0.0094\n",
      "Variable: 1043                 Importance: 0.0094\n",
      "Variable: 750                  Importance: 0.0091\n",
      "Variable: 752                  Importance: 0.0091\n",
      "Variable: 1064                 Importance: 0.0091\n",
      "Variable: 960                  Importance: 0.009\n",
      "Variable: 1039                 Importance: 0.0084\n",
      "Variable: 783                  Importance: 0.0081\n",
      "Variable: 991                  Importance: 0.0081\n",
      "Variable: 1046                 Importance: 0.0081\n",
      "Variable: 854                  Importance: 0.0079\n",
      "Variable: 843                  Importance: 0.0078\n",
      "Variable: 1015                 Importance: 0.0075\n",
      "Variable: 1017                 Importance: 0.0073\n",
      "Variable: 844                  Importance: 0.0072\n",
      "Variable: 1020                 Importance: 0.0072\n",
      "Variable: 756                  Importance: 0.0071\n",
      "Variable: 914                  Importance: 0.0069\n",
      "Variable: 830                  Importance: 0.0068\n",
      "Variable: 942                  Importance: 0.0068\n",
      "Variable: 979                  Importance: 0.0067\n",
      "Variable: 865                  Importance: 0.0066\n",
      "Variable: 859                  Importance: 0.0065\n",
      "Variable: 789                  Importance: 0.0064\n",
      "Variable: 994                  Importance: 0.0063\n",
      "Variable: 936                  Importance: 0.0062\n",
      "Variable: 983                  Importance: 0.0061\n",
      "Variable: 1000                 Importance: 0.0061\n",
      "Variable: 892                  Importance: 0.006\n",
      "Variable: 1023                 Importance: 0.006\n",
      "Variable: 1056                 Importance: 0.006\n",
      "Variable: 924                  Importance: 0.0059\n",
      "Variable: 1019                 Importance: 0.0059\n",
      "Variable: 930                  Importance: 0.0057\n",
      "Variable: 1014                 Importance: 0.0057\n",
      "Variable: 837                  Importance: 0.0056\n",
      "Variable: 852                  Importance: 0.0056\n",
      "Variable: 805                  Importance: 0.0055\n",
      "Variable: 849                  Importance: 0.0055\n",
      "Variable: 866                  Importance: 0.0054\n",
      "Variable: 933                  Importance: 0.0054\n",
      "Variable: 949                  Importance: 0.0054\n",
      "Variable: 978                  Importance: 0.0054\n",
      "Variable: 846                  Importance: 0.0053\n",
      "Variable: 950                  Importance: 0.0053\n",
      "Variable: 1016                 Importance: 0.0053\n",
      "Variable: 797                  Importance: 0.0052\n",
      "Variable: 993                  Importance: 0.0052\n",
      "Variable: 787                  Importance: 0.0051\n",
      "Variable: 742                  Importance: 0.005\n",
      "Variable: 809                  Importance: 0.005\n",
      "Variable: 826                  Importance: 0.005\n",
      "Variable: 1002                 Importance: 0.005\n",
      "Variable: 775                  Importance: 0.0049\n",
      "Variable: 780                  Importance: 0.0049\n",
      "Variable: 799                  Importance: 0.0049\n",
      "Variable: 941                  Importance: 0.0049\n",
      "Variable: 946                  Importance: 0.0047\n",
      "Variable: 985                  Importance: 0.0047\n",
      "Variable: 1018                 Importance: 0.0047\n",
      "Variable: 1042                 Importance: 0.0047\n",
      "Variable: 1052                 Importance: 0.0047\n",
      "Variable: 753                  Importance: 0.0046\n",
      "Variable: 915                  Importance: 0.0046\n",
      "Variable: 1040                 Importance: 0.0046\n",
      "Variable: 1054                 Importance: 0.0046\n",
      "Variable: 761                  Importance: 0.0045\n",
      "Variable: 923                  Importance: 0.0045\n",
      "Variable: 744                  Importance: 0.0044\n",
      "Variable: 758                  Importance: 0.0044\n",
      "Variable: 882                  Importance: 0.0044\n",
      "Variable: 980                  Importance: 0.0044\n",
      "Variable: 988                  Importance: 0.0044\n",
      "Variable: 1057                 Importance: 0.0044\n",
      "Variable: 765                  Importance: 0.0043\n",
      "Variable: 808                  Importance: 0.0043\n",
      "Variable: 842                  Importance: 0.0043\n",
      "Variable: 858                  Importance: 0.0043\n",
      "Variable: 834                  Importance: 0.0042\n",
      "Variable: 907                  Importance: 0.0042\n",
      "Variable: 776                  Importance: 0.0041\n",
      "Variable: 782                  Importance: 0.0041\n",
      "Variable: 831                  Importance: 0.0041\n",
      "Variable: 851                  Importance: 0.0041\n",
      "Variable: 922                  Importance: 0.0041\n",
      "Variable: 1013                 Importance: 0.0041\n",
      "Variable: 740                  Importance: 0.004\n",
      "Variable: 827                  Importance: 0.004\n",
      "Variable: 976                  Importance: 0.004\n",
      "Variable: 998                  Importance: 0.004\n",
      "Variable: 888                  Importance: 0.0039\n",
      "Variable: 972                  Importance: 0.0038\n",
      "Variable: 1050                 Importance: 0.0038\n",
      "Variable: 812                  Importance: 0.0037\n",
      "Variable: 819                  Importance: 0.0037\n",
      "Variable: 853                  Importance: 0.0037\n",
      "Variable: 956                  Importance: 0.0037\n",
      "Variable: 959                  Importance: 0.0037\n",
      "Variable: 961                  Importance: 0.0037\n",
      "Variable: 996                  Importance: 0.0037\n",
      "Variable: 777                  Importance: 0.0036\n",
      "Variable: 779                  Importance: 0.0036\n",
      "Variable: 970                  Importance: 0.0036\n",
      "Variable: 751                  Importance: 0.0035\n",
      "Variable: 798                  Importance: 0.0035\n",
      "Variable: 818                  Importance: 0.0035\n",
      "Variable: 964                  Importance: 0.0035\n",
      "Variable: 1053                 Importance: 0.0035\n",
      "Variable: 781                  Importance: 0.0034\n",
      "Variable: 786                  Importance: 0.0034\n",
      "Variable: 811                  Importance: 0.0034\n",
      "Variable: 841                  Importance: 0.0034\n",
      "Variable: 969                  Importance: 0.0034\n",
      "Variable: 1041                 Importance: 0.0034\n",
      "Variable: 832                  Importance: 0.0033\n",
      "Variable: 885                  Importance: 0.0033\n",
      "Variable: 927                  Importance: 0.0033\n",
      "Variable: 943                  Importance: 0.0033\n",
      "Variable: 989                  Importance: 0.0033\n",
      "Variable: 1051                 Importance: 0.0033\n",
      "Variable: 816                  Importance: 0.0031\n",
      "Variable: 833                  Importance: 0.0031\n",
      "Variable: 876                  Importance: 0.0031\n",
      "Variable: 894                  Importance: 0.0031\n",
      "Variable: 939                  Importance: 0.0031\n",
      "Variable: 954                  Importance: 0.0031\n",
      "Variable: 806                  Importance: 0.003\n",
      "Variable: 822                  Importance: 0.003\n",
      "Variable: 839                  Importance: 0.003\n",
      "Variable: 863                  Importance: 0.003\n",
      "Variable: 913                  Importance: 0.003\n",
      "Variable: 1001                 Importance: 0.003\n",
      "Variable: 764                  Importance: 0.0029\n",
      "Variable: 785                  Importance: 0.0029\n",
      "Variable: 804                  Importance: 0.0029\n",
      "Variable: 875                  Importance: 0.0029\n",
      "Variable: 903                  Importance: 0.0029\n",
      "Variable: 986                  Importance: 0.0029\n",
      "Variable: 807                  Importance: 0.0028\n",
      "Variable: 823                  Importance: 0.0028\n",
      "Variable: 871                  Importance: 0.0028\n",
      "Variable: 965                  Importance: 0.0028\n",
      "Variable: 981                  Importance: 0.0028\n",
      "Variable: 982                  Importance: 0.0028\n",
      "Variable: 1028                 Importance: 0.0028\n",
      "Variable: 743                  Importance: 0.0027\n",
      "Variable: 792                  Importance: 0.0027\n",
      "Variable: 800                  Importance: 0.0027\n",
      "Variable: 868                  Importance: 0.0027\n",
      "Variable: 883                  Importance: 0.0027\n",
      "Variable: 968                  Importance: 0.0027\n",
      "Variable: 974                  Importance: 0.0027\n",
      "Variable: 749                  Importance: 0.0026\n",
      "Variable: 774                  Importance: 0.0026\n",
      "Variable: 784                  Importance: 0.0026\n",
      "Variable: 870                  Importance: 0.0026\n",
      "Variable: 925                  Importance: 0.0026\n",
      "Variable: 944                  Importance: 0.0026\n",
      "Variable: 1003                 Importance: 0.0026\n",
      "Variable: 794                  Importance: 0.0025\n",
      "Variable: 916                  Importance: 0.0025\n",
      "Variable: 928                  Importance: 0.0025\n",
      "Variable: 769                  Importance: 0.0024\n",
      "Variable: 771                  Importance: 0.0024\n",
      "Variable: 1022                 Importance: 0.0024\n",
      "Variable: 748                  Importance: 0.0023\n",
      "Variable: 791                  Importance: 0.0023\n",
      "Variable: 860                  Importance: 0.0023\n",
      "Variable: 884                  Importance: 0.0023\n",
      "Variable: 890                  Importance: 0.0023\n",
      "Variable: 801                  Importance: 0.0022\n",
      "Variable: 802                  Importance: 0.0022\n",
      "Variable: 904                  Importance: 0.0022\n",
      "Variable: 952                  Importance: 0.0022\n",
      "Variable: 958                  Importance: 0.0022\n",
      "Variable: 1038                 Importance: 0.0022\n",
      "Variable: 1045                 Importance: 0.0022\n",
      "Variable: 856                  Importance: 0.0021\n",
      "Variable: 931                  Importance: 0.0021\n",
      "Variable: 962                  Importance: 0.0021\n",
      "Variable: 762                  Importance: 0.002\n",
      "Variable: 773                  Importance: 0.002\n",
      "Variable: 813                  Importance: 0.002\n",
      "Variable: 825                  Importance: 0.002\n",
      "Variable: 835                  Importance: 0.002\n",
      "Variable: 850                  Importance: 0.002\n",
      "Variable: 886                  Importance: 0.002\n",
      "Variable: 908                  Importance: 0.002\n",
      "Variable: 977                  Importance: 0.002\n",
      "Variable: 1069                 Importance: 0.002\n",
      "Variable: 847                  Importance: 0.0019\n",
      "Variable: 874                  Importance: 0.0019\n",
      "Variable: 895                  Importance: 0.0019\n",
      "Variable: 963                  Importance: 0.0019\n",
      "Variable: 987                  Importance: 0.0019\n",
      "Variable: 995                  Importance: 0.0019\n",
      "Variable: 1068                 Importance: 0.0019\n",
      "Variable: 755                  Importance: 0.0018\n",
      "Variable: 767                  Importance: 0.0018\n",
      "Variable: 796                  Importance: 0.0018\n",
      "Variable: 867                  Importance: 0.0018\n",
      "Variable: 929                  Importance: 0.0018\n",
      "Variable: 951                  Importance: 0.0018\n",
      "Variable: 971                  Importance: 0.0018\n",
      "Variable: 990                  Importance: 0.0018\n",
      "Variable: 1021                 Importance: 0.0018\n",
      "Variable: 1026                 Importance: 0.0018\n",
      "Variable: 1060                 Importance: 0.0018\n",
      "Variable: 741                  Importance: 0.0017\n",
      "Variable: 862                  Importance: 0.0017\n",
      "Variable: 897                  Importance: 0.0017\n",
      "Variable: 967                  Importance: 0.0017\n",
      "Variable: 997                  Importance: 0.0017\n",
      "Variable: 1010                 Importance: 0.0017\n",
      "Variable: 815                  Importance: 0.0016\n",
      "Variable: 873                  Importance: 0.0016\n",
      "Variable: 878                  Importance: 0.0016\n",
      "Variable: 788                  Importance: 0.0015\n",
      "Variable: 803                  Importance: 0.0015\n",
      "Variable: 836                  Importance: 0.0015\n",
      "Variable: 887                  Importance: 0.0015\n",
      "Variable: 901                  Importance: 0.0015\n",
      "Variable: 1032                 Importance: 0.0015\n",
      "Variable: 1058                 Importance: 0.0015\n",
      "Variable: 760                  Importance: 0.0014\n",
      "Variable: 861                  Importance: 0.0014\n",
      "Variable: 912                  Importance: 0.0014\n",
      "Variable: 999                  Importance: 0.0014\n",
      "Variable: 1005                 Importance: 0.0014\n",
      "Variable: 1006                 Importance: 0.0014\n",
      "Variable: 772                  Importance: 0.0013\n",
      "Variable: 857                  Importance: 0.0013\n",
      "Variable: 880                  Importance: 0.0013\n",
      "Variable: 891                  Importance: 0.0013\n",
      "Variable: 896                  Importance: 0.0013\n",
      "Variable: 899                  Importance: 0.0013\n",
      "Variable: 973                  Importance: 0.0013\n",
      "Variable: 984                  Importance: 0.0013\n",
      "Variable: 1035                 Importance: 0.0013\n",
      "Variable: 1047                 Importance: 0.0013\n",
      "Variable: 1049                 Importance: 0.0013\n",
      "Variable: 1055                 Importance: 0.0013\n",
      "Variable: 754                  Importance: 0.0012\n",
      "Variable: 864                  Importance: 0.0012\n",
      "Variable: 869                  Importance: 0.0012\n",
      "Variable: 906                  Importance: 0.0012\n",
      "Variable: 940                  Importance: 0.0012\n",
      "Variable: 945                  Importance: 0.0012\n",
      "Variable: 1037                 Importance: 0.0012\n",
      "Variable: 746                  Importance: 0.0011\n",
      "Variable: 909                  Importance: 0.0011\n",
      "Variable: 921                  Importance: 0.0011\n",
      "Variable: 947                  Importance: 0.0011\n",
      "Variable: 1004                 Importance: 0.0011\n",
      "Variable: 1025                 Importance: 0.0011\n",
      "Variable: 1027                 Importance: 0.0011\n",
      "Variable: 1063                 Importance: 0.0011\n",
      "Variable: 759                  Importance: 0.001\n",
      "Variable: 768                  Importance: 0.001\n",
      "Variable: 889                  Importance: 0.001\n",
      "Variable: 898                  Importance: 0.001\n",
      "Variable: 935                  Importance: 0.001\n",
      "Variable: 1024                 Importance: 0.001\n",
      "Variable: 770                  Importance: 0.0009\n",
      "Variable: 814                  Importance: 0.0009\n",
      "Variable: 845                  Importance: 0.0009\n",
      "Variable: 905                  Importance: 0.0009\n",
      "Variable: 910                  Importance: 0.0009\n",
      "Variable: 920                  Importance: 0.0009\n",
      "Variable: 957                  Importance: 0.0009\n",
      "Variable: 1031                 Importance: 0.0009\n",
      "Variable: 824                  Importance: 0.0008\n",
      "Variable: 881                  Importance: 0.0008\n",
      "Variable: 917                  Importance: 0.0008\n",
      "Variable: 918                  Importance: 0.0008\n",
      "Variable: 1009                 Importance: 0.0008\n",
      "Variable: 1036                 Importance: 0.0008\n",
      "Variable: 1044                 Importance: 0.0008\n",
      "Variable: 766                  Importance: 0.0007\n",
      "Variable: 829                  Importance: 0.0007\n",
      "Variable: 840                  Importance: 0.0007\n",
      "Variable: 877                  Importance: 0.0007\n",
      "Variable: 948                  Importance: 0.0007\n",
      "Variable: 1007                 Importance: 0.0007\n",
      "Variable: 1012                 Importance: 0.0007\n",
      "Variable: 1066                 Importance: 0.0007\n",
      "Variable: 763                  Importance: 0.0006\n",
      "Variable: 795                  Importance: 0.0006\n",
      "Variable: 810                  Importance: 0.0006\n",
      "Variable: 828                  Importance: 0.0006\n",
      "Variable: 838                  Importance: 0.0006\n",
      "Variable: 934                  Importance: 0.0006\n",
      "Variable: 953                  Importance: 0.0006\n",
      "Variable: 1030                 Importance: 0.0006\n",
      "Variable: 1062                 Importance: 0.0006\n",
      "Variable: 778                  Importance: 0.0005\n",
      "Variable: 793                  Importance: 0.0005\n",
      "Variable: 893                  Importance: 0.0005\n",
      "Variable: 900                  Importance: 0.0005\n",
      "Variable: 902                  Importance: 0.0005\n",
      "Variable: 932                  Importance: 0.0005\n",
      "Variable: 937                  Importance: 0.0005\n",
      "Variable: 992                  Importance: 0.0005\n",
      "Variable: 1029                 Importance: 0.0005\n",
      "Variable: 1048                 Importance: 0.0005\n",
      "Variable: 745                  Importance: 0.0004\n",
      "Variable: 821                  Importance: 0.0004\n",
      "Variable: 855                  Importance: 0.0004\n",
      "Variable: 872                  Importance: 0.0004\n",
      "Variable: 911                  Importance: 0.0004\n",
      "Variable: 926                  Importance: 0.0004\n",
      "Variable: 966                  Importance: 0.0004\n",
      "Variable: 1011                 Importance: 0.0004\n",
      "Variable: 1070                 Importance: 0.0004\n",
      "Variable: 817                  Importance: 0.0003\n",
      "Variable: 747                  Importance: 0.0002\n",
      "Variable: 919                  Importance: 0.0002\n",
      "Variable: 938                  Importance: 0.0002\n",
      "Variable: 1008                 Importance: 0.0002\n",
      "Variable: 1033                 Importance: 0.0002\n",
      "Variable: 1061                 Importance: 0.0001\n",
      "Variable: 820                  Importance: 0.0\n",
      "Variable: 879                  Importance: 0.0\n",
      "Variable: 955                  Importance: 0.0\n",
      "Variable: 1034                 Importance: 0.0\n",
      "Variable: 1067                 Importance: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rf.fit(X_train, y_train)\n",
    "# Get numerical feature importances\n",
    "importances = list(Rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 4)) for feature, importance in zip(X_col, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion:- Random Forest is giving accuracy of 80% using all 331 variables. If we reduce Variables using techineque Recursive Feature Elimination(RFE),using 17 variables model is giving acurracy of 77%. Further increase in Variables will not affecting model accuracy as much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
